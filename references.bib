@article{avrahamRetinalProstheticVision2021,
  title = {Retinal Prosthetic Vision Simulation: Temporal Aspects},
  shorttitle = {Retinal Prosthetic Vision Simulation},
  author = {Avraham, David and Jung, Jae-Hyun and Yitzhaky, Yitzhak and Peli, Eli},
  date = {2021-08},
  journaltitle = {Journal of Neural Engineering},
  shortjournal = {J. Neural Eng.},
  volume = {18},
  number = {4},
  pages = {0460d9},
  publisher = {IOP Publishing},
  issn = {1741-2552},
  doi = {10.1088/1741-2552/ac1b6c},
  url = {https://dx.doi.org/10.1088/1741-2552/ac1b6c},
  urldate = {2024-06-20},
  abstract = {Objective. The perception of individuals fitted with retinal prostheses is not fully understood, although several retinal implants have been tested and commercialized. Realistic simulations of perception with retinal implants would be useful for future development and evaluation of such systems. Approach. We implemented a retinal prosthetic vision simulation, including temporal features, which have not been previously simulated. In particular, the simulation included temporal aspects such as persistence and perceptual fading of phosphenes and the electrode activation rate. Main results. The simulated phosphene persistence showed an effective reduction in flickering at low electrode activation rates. Although persistence has a positive effect on static scenes, it smears dynamic scenes. Perceptual fading following continuous stimulation affects prosthetic vision of both static and dynamic scenes by making them disappear completely or partially. However, we showed that perceptual fading of a static stimulus might be countered by head-scanning motions, which together with the persistence revealed the contours of the faded object. We also showed that changing the image polarity may improve simulated prosthetic vision in the presence of persistence and perceptual fading. Significance. Temporal aspects have important roles in prosthetic vision, as illustrated by the simulations. Considering these aspects may improve the future design, the training with, and evaluation of retinal prostheses.},
  langid = {english},
  file = {C:\Users\marc_\Zotero\storage\BW9Z4NYT\Avraham e.a. - 2021 - Retinal prosthetic vision simulation temporal asp.pdf}
}

@incollection{baianoVisualEvokedPotential2024,
  title = {Visual {{Evoked Potential}}},
  booktitle = {{{StatPearls}}},
  author = {Baiano, Cinzia and Zeppieri, Marco},
  date = {2024},
  eprint = {35881733},
  eprinttype = {pmid},
  publisher = {StatPearls Publishing},
  location = {Treasure Island (FL)},
  url = {http://www.ncbi.nlm.nih.gov/books/NBK582128/},
  urldate = {2024-07-02},
  abstract = {Visual evoked potentials (VEP)s represent a valid electrophysiological tool in neurological pathologies. VEPs are the expression of the electrical activity of the visual pathways up to the optic nerve to the calcarine cortex. These potentials provide a non-invasive exploration method of the functionality of the human visual system through detecting neuronal pool activity responding to stimuli independently of the consciousness and attention state of the patient. The measurement of VEP is made possible by applying electrodes on the scalp in the occipital region and administering visual stimuli in a patient with open eyes.~ Depending on the characteristics of the stimulus, pattern visual evoked potentials can provide different information on the functionality of the various sectors of the visual field and the integrity of the optical pathways.~This type of instrumental exploration has a clinical application in ophthalmological retinal pathology and neurological pathology related to the optic nerve and/or brain (inflammatory, atrophic, toxic, tumoral, and genetic disease). VEPs has been used as an alternative method to assess visual acuity in non-verbal infants and adults with low intellectual abilities or potential malingering.},
  langid = {english},
  file = {C:\Users\marc_\Zotero\storage\LYYPKFCE\NBK582128.html}
}

@article{beauchampDynamicStimulationVisual2020,
  title = {Dynamic {{Stimulation}} of {{Visual Cortex Produces Form Vision}} in {{Sighted}} and {{Blind Humans}}},
  author = {Beauchamp, Michael S. and Oswalt, Denise and Sun, Ping and Foster, Brett L. and Magnotti, John F. and Niketeghad, Soroush and Pouratian, Nader and Bosking, William H. and Yoshor, Daniel},
  date = {2020-05},
  journaltitle = {Cell},
  shortjournal = {Cell},
  volume = {181},
  number = {4},
  pages = {774-783.e5},
  issn = {00928674},
  doi = {10.1016/j.cell.2020.04.033},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0092867420304967},
  urldate = {2024-06-20},
  abstract = {A visual cortical prosthesis (VCP) has long been proposed as a strategy for restoring useful vision to the blind, under the assumption that visual percepts of small spots of light produced with electrical stimulation of visual cortex (phosphenes) will combine into coherent percepts of visual forms, like pixels on a video screen. We tested an alternative strategy in which shapes were traced on the surface of visual cortex by stimulating electrodes in dynamic sequence. In both sighted and blind participants, dynamic stimulation enabled accurate recognition of letter shapes predicted by the brain’s spatial map of the visual world. Forms were presented and recognized rapidly by blind participants, up to 86 forms per minute. These findings demonstrate that a brain prosthetic can produce coherent percepts of visual forms.},
  langid = {english},
  file = {C:\Users\marc_\Zotero\storage\C5S3XMLI\Beauchamp e.a. - 2020 - Dynamic Stimulation of Visual Cortex Produces Form.pdf}
}

@article{bourneTrendsPrevalenceBlindness2021,
  title = {Trends in Prevalence of Blindness and Distance and near Vision Impairment over 30 Years: An Analysis for the {{Global Burden}} of {{Disease Study}}},
  shorttitle = {Trends in Prevalence of Blindness and Distance and near Vision Impairment over 30 Years},
  author = {Bourne, Rupert and Steinmetz, Jaimie D. and Flaxman, Seth and Briant, Paul Svitil and Taylor, Hugh R. and Resnikoff, Serge and Casson, Robert James and Abdoli, Amir and Abu-Gharbieh, Eman and Afshin, Ashkan and Ahmadieh, Hamid and Akalu, Yonas and Alamneh, Alehegn Aderaw and Alemayehu, Wondu and Alfaar, Ahmed Samir and Alipour, Vahid and Anbesu, Etsay Woldu and Androudi, Sofia and Arabloo, Jalal and Arditi, Aries and Asaad, Malke and Bagli, Eleni and Baig, Atif Amin and Bärnighausen, Till Winfried and Parodi, Maurizio Battaglia and Bhagavathula, Akshaya Srikanth and Bhardwaj, Nikha and Bhardwaj, Pankaj and Bhattacharyya, Krittika and Bijani, Ali and Bikbov, Mukharram and Bottone, Michele and Braithwaite, Tasanee and Bron, Alain M. and Butt, Zahid A. and Cheng, Ching-Yu and Chu, Dinh-Toi and Cicinelli, Maria Vittoria and Coelho, João M. and Dagnew, Baye and Dai, Xiaochen and Dana, Reza and Dandona, Lalit and Dandona, Rakhi and Monte, Monte A. Del and Deva, Jenny P. and Diaz, Daniel and Djalalinia, Shirin and Dreer, Laura E. and Ehrlich, Joshua R. and Ellwein, Leon B. and Emamian, Mohammad Hassan and Fernandes, Arthur G. and Fischer, Florian and Friedman, David S. and Furtado, João M. and Gaidhane, Abhay Motiramji and Gaidhane, Shilpa and Gazzard, Gus and Gebremichael, Berhe and George, Ronnie and Ghashghaee, Ahmad and Golechha, Mahaveer and Hamidi, Samer and Hammond, Billy Randall and Hartnett, Mary Elizabeth R. and Hartono, Risky Kusuma and Hay, Simon I. and Heidari, Golnaz and Ho, Hung Chak and Hoang, Chi Linh and Househ, Mowafa and Ibitoye, Segun Emmanuel and Ilic, Irena M. and Ilic, Milena D. and Ingram, April D. and Irvani, Seyed Sina Naghibi and Jha, Ravi Prakash and Kahloun, Rim and Kandel, Himal and Kasa, Ayele Semachew and Kempen, John H. and Keramati, Maryam and Khairallah, Moncef and Khan, Ejaz Ahmad and Khanna, Rohit C. and Khatib, Mahalaqua Nazli and Kim, Judy E. and Kim, Yun Jin and Kisa, Sezer and Kisa, Adnan and Koyanagi, Ai and Kurmi, Om P. and Lansingh, Van Charles and Leasher, Janet L. and Leveziel, Nicolas and Limburg, Hans and Majdan, Marek and Manafi, Navid and Mansouri, Kaweh and McAlinden, Colm and Mohammadi, Seyed Farzad and Mohammadian-Hafshejani, Abdollah and Mohammadpourhodki, Reza and Mokdad, Ali H. and Moosavi, Delaram and Morse, Alan R. and Naderi, Mehdi and Naidoo, Kovin S. and Nangia, Vinay and Nguyen, Cuong Tat and Nguyen, Huong Lan Thi and Ogundimu, Kolawole and Olagunju, Andrew T. and Ostroff, Samuel M. and Panda-Jonas, Songhomitra and Pesudovs, Konrad and Peto, Tunde and Syed, Zahiruddin Quazi and Rahman, Mohammad Hifz Ur and Ramulu, Pradeep Y. and Rawaf, Salman and Rawaf, David Laith and Reinig, Nickolas and Robin, Alan L. and Rossetti, Luca and Safi, Sare and Sahebkar, Amirhossein and Samy, Abdallah M. and Saxena, Deepak and Serle, Janet B. and Shaikh, Masood Ali and Shen, Tueng T. and Shibuya, Kenji and Shin, Jae Il and Silva, Juan Carlos and Silvester, Alexander and Singh, Jasvinder A. and Singhal, Deepika and Sitorus, Rita S. and Skiadaresi, Eirini and Skirbekk, Vegard and Soheili, Amin and Sousa, Raúl A. R. C. and Spurlock, Emma Elizabeth and Stambolian, Dwight and Taddele, Biruk Wogayehu and Tadesse, Eyayou Girma and Tahhan, Nina and Tareque, Md Ismail and Topouzis, Fotis and Tran, Bach Xuan and Travillian, Ravensara S. and Tsilimbaris, Miltiadis K. and Varma, Rohit and Virgili, Gianni and Wang, Ya Xing and Wang, Ningli and West, Sheila K. and Wong, Tien Y. and Zaidi, Zoubida and Zewdie, Kaleab Alemayehu and Jonas, Jost B. and Vos, Theo},
  date = {2021-02-01},
  journaltitle = {The Lancet Global Health},
  shortjournal = {The Lancet Global Health},
  volume = {9},
  number = {2},
  eprint = {33275950},
  eprinttype = {pmid},
  pages = {e130-e143},
  publisher = {Elsevier},
  issn = {2214-109X},
  doi = {10.1016/S2214-109X(20)30425-3},
  url = {https://www.thelancet.com/journals/langlo/article/PIIS2214-109X(20)30425-3/fulltext},
  urldate = {2024-05-29},
  langid = {english},
  file = {C:\Users\marc_\Zotero\storage\AQF8CS6R\Bourne e.a. - 2021 - Trends in prevalence of blindness and distance and.pdf}
}

@article{chenNeuralCorrelatesObstacle2017,
  title = {Neural Correlates of Obstacle Negotiation in Older Adults: {{An fNIRS}} Study},
  shorttitle = {Neural Correlates of Obstacle Negotiation in Older Adults},
  author = {Chen, Michelle and Pillemer, Sarah and England, Sarah and Izzetoglu, Meltem and Mahoney, Jeannette R. and Holtzer, Roee},
  date = {2017-10},
  journaltitle = {Gait \& Posture},
  shortjournal = {Gait \& Posture},
  volume = {58},
  pages = {130--135},
  issn = {09666362},
  doi = {10.1016/j.gaitpost.2017.07.043},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0966636217307464},
  urldate = {2024-07-03},
  abstract = {Older adults are less efficient at avoiding obstacles compared to young adults, especially under attention-demanding conditions. Using functional near-infrared-spectroscopy (fNIRS), recent studies implicated the prefrontal cortex (PFC) in cognitive control of locomotion, notably under dual-task walking conditions. The neural substrates underlying Obstacle Negotiation (ON), however, have not been established. The current study determined the role of the PFC in ON during walking in seniors. Non-demented older adults (n = 90; mean age = 78.1 ± 5.5 years; \%female = 51) underwent fNIRS acquisition to assess changes in hemodynamic activity in the PFC during normal-walk [NW] and walk-while-talk [WWT] conditions with and without obstacles. Obstacles were presented as red elliptical shapes using advanced laser technology, which resemble potholes. Linear mixed effects models were used to determine differences in oxygenated hemoglobin (HbO2) levels among the four task conditions. The presence of slow gait, a risk factor for dementia and falls, served as a predictor hypothesized to moderate the effect of obstacles on PFC HbO2 levels. PFC HbO2 levels were significantly higher in WWT compared to NW (p {$<$} 0.001) irrespective of ON. Slow gait moderated the effect of obstacles on HbO2 levels across task conditions. Specifically, compared to participants with normal gait, PFC HbO2 levels were significantly increased in ON-NW relative to NW (p = 0.017) and ON-WWT relative to WWT (p {$<$} 0.001) among individuals with slow gait. Consistent with Compensatory Reallocation, ON required greater PFC involvement among individuals with mobility limitations.},
  langid = {english},
  file = {C:\Users\marc_\Zotero\storage\I8HNITZ5\Chen e.a. - 2017 - Neural correlates of obstacle negotiation in older.pdf}
}

@article{chenShapePerceptionHighchannelcount2020,
  title = {Shape Perception via a High-Channel-Count Neuroprosthesis in Monkey Visual Cortex},
  author = {Chen, Xing and Wang, Feng and Fernandez, Eduardo and Roelfsema, Pieter R.},
  date = {2020-12-04},
  journaltitle = {Science},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.abd7435},
  url = {https://www.science.org/doi/10.1126/science.abd7435},
  urldate = {2024-05-27},
  abstract = {Electrical stimulation of the visual cortex with a neuroprosthetic device allows artificial vision with shape and motion perception.},
  langid = {english},
  file = {C\:\\Users\\marc_\\Zotero\\storage\\N9DMMR6V\\Chen e.a. - 2020 - Shape perception via a high-channel-count neuropro.pdf;C\:\\Users\\marc_\\Zotero\\storage\\M3SHNKWU\\science.html}
}

@article{deruytervansteveninckEndtoendOptimizationProsthetic2022,
  title = {End-to-End Optimization of Prosthetic Vision},
  author = {family=Ruyter van Steveninck, given=Jaap, prefix=de, useprefix=true and Güçlü, Umut and family=Wezel, given=Richard, prefix=van, useprefix=true and family=Gerven, given=Marcel, prefix=van, useprefix=true},
  date = {2022-02-28},
  journaltitle = {Journal of Vision},
  shortjournal = {Journal of Vision},
  volume = {22},
  number = {2},
  pages = {20},
  issn = {1534-7362},
  doi = {10.1167/jov.22.2.20},
  url = {https://doi.org/10.1167/jov.22.2.20},
  urldate = {2024-05-10},
  abstract = {Neural prosthetics may provide a promising solution to restore visual perception in some forms of blindness. The restored prosthetic percept is rudimentary compared to normal vision and can be optimized with a variety of image preprocessing techniques to maximize relevant information transfer. Extracting the most useful features from a visual scene is a nontrivial task and optimal preprocessing choices strongly depend on the context. Despite rapid advancements in deep learning, research currently faces a difficult challenge in finding a general and automated preprocessing strategy that can be tailored to specific tasks or user requirements. In this paper, we present a novel deep learning approach that explicitly addresses this issue by optimizing the entire process of phosphene generation in an end-to-end fashion. The proposed model is based on a deep auto-encoder architecture and includes a highly adjustable simulation module of prosthetic vision. In computational validation experiments, we show that such an approach is able to automatically find a task-specific stimulation protocol. The results of these proof-of-principle experiments illustrate the potential of end-to-end optimization for prosthetic vision. The presented approach is highly modular and our approach could be extended to automated dynamic optimization of prosthetic vision for everyday tasks, given any specific constraints, accommodating individual requirements of the end-user.},
  file = {C\:\\Users\\marc_\\Zotero\\storage\\D2H6LCK2\\de Ruyter van Steveninck e.a. - 2022 - End-to-end optimization of prosthetic vision.pdf;C\:\\Users\\marc_\\Zotero\\storage\\859NWS4K\\article.html}
}

@article{deruytervansteveninckRealworldIndoorMobility2022,
  title = {Real-World Indoor Mobility with Simulated Prosthetic Vision: {{The}} Benefits and Feasibility of Contour-Based Scene Simplification at Different Phosphene Resolutions},
  shorttitle = {Real-World Indoor Mobility with Simulated Prosthetic Vision},
  author = {family=Ruyter van Steveninck, given=Jaap, prefix=de, useprefix=true and family=Gestel, given=Tom, prefix=van, useprefix=true and Koenders, Paula and family=Ham, given=Guus, prefix=van der, useprefix=true and Vereecken, Floris and Güçlü, Umut and family=Gerven, given=Marcel, prefix=van, useprefix=true and Güçlütürk, Yağmur and family=Wezel, given=Richard, prefix=van, useprefix=true},
  date = {2022-02-01},
  journaltitle = {Journal of Vision},
  shortjournal = {Journal of Vision},
  volume = {22},
  number = {2},
  pages = {1},
  issn = {1534-7362},
  doi = {10.1167/jov.22.2.1},
  url = {https://doi.org/10.1167/jov.22.2.1},
  urldate = {2024-05-11},
  abstract = {Neuroprosthetic implants are a promising technology for restoring some form of vision in people with visual impairments via electrical neurostimulation in the visual pathway. Although an artificially generated prosthetic percept is relatively limited compared with normal vision, it may provide some elementary perception of the surroundings, re-enabling daily living functionality. For mobility in particular, various studies have investigated the benefits of visual neuroprosthetics in a simulated prosthetic vision paradigm with varying outcomes. The previous literature suggests that scene simplification via image processing, and particularly contour extraction, may potentially improve the mobility performance in a virtual environment. In the current simulation study with sighted participants, we explore both the theoretically attainable benefits of strict scene simplification in an indoor environment by controlling the environmental complexity, as well as the practically achieved improvement with a deep learning-based surface boundary detection implementation compared with traditional edge detection. A simulated electrode resolution of 26 × 26 was found to provide sufficient information for mobility in a simple environment. Our results suggest that, for a lower number of implanted electrodes, the removal of background textures and within-surface gradients may be beneficial in theory. However, the deep learning-based implementation for surface boundary detection did not improve mobility performance in the current study. Furthermore, our findings indicate that, for a greater number of electrodes, the removal of within-surface gradients and background textures may deteriorate, rather than improve, mobility. Therefore, finding a balanced amount of scene simplification requires a careful tradeoff between informativity and interpretability that may depend on the number of implanted electrodes.},
  file = {C\:\\Users\\marc_\\Zotero\\storage\\FZYDCCTU\\de Ruyter van Steveninck e.a. - 2022 - Real-world indoor mobility with simulated prosthet.pdf;C\:\\Users\\marc_\\Zotero\\storage\\J7IKJKKU\\article.html}
}

@article{elnabawyPVGANGenerativeAdversarial2022,
  title = {{{PVGAN}}: A Generative Adversarial Network for Object Simplification in Prosthetic Vision},
  shorttitle = {{{PVGAN}}},
  author = {Elnabawy, Reham H. and Abdennadher, Slim and Hellwich, Olaf and Eldawlatly, Seif},
  date = {2022-09},
  journaltitle = {Journal of Neural Engineering},
  shortjournal = {J. Neural Eng.},
  volume = {19},
  number = {5},
  pages = {056007},
  publisher = {IOP Publishing},
  issn = {1741-2552},
  doi = {10.1088/1741-2552/ac8acf},
  url = {https://dx.doi.org/10.1088/1741-2552/ac8acf},
  urldate = {2024-05-22},
  abstract = {Objective. By means of electrical stimulation of the visual system, visual prostheses provide promising solution for blind patients through partial restoration of their vision. Despite the great success achieved so far in this field, the limited resolution of the perceived vision using these devices hinders the ability of visual prostheses users to correctly recognize viewed objects. Accordingly, we propose a deep learning approach based on generative adversarial networks (GANs), termed prosthetic vision GAN (PVGAN), to enhance object recognition for the implanted patients by representing objects in the field of view based on a corresponding simplified clip art version. Approach. To assess the performance, an axon map model was used to simulate prosthetic vision in experiments involving normally-sighted participants. In these experiments, four types of image representation were examined. The first and second types comprised presenting phosphene simulation of real images containing the actual high-resolution object, and presenting phosphene simulation of the real image followed by the clip art image, respectively. The other two types were utilized to evaluate the performance in the case of electrode dropout, where the third type comprised presenting phosphene simulation of only clip art images without electrode dropout, while the fourth type involved clip art images with electrode dropout. Main results. The performance was measured through three evaluation metrics which are the accuracy of the participants in recognizing the objects, the time taken by the participants to correctly recognize the object, and the confidence level of the participants in the recognition process. Results demonstrate that representing the objects using clip art images generated by the PVGAN model results in a significant enhancement in the speed and confidence of the subjects in recognizing the objects. Significance. These results demonstrate the utility of using GANs in enhancing the quality of images perceived using prosthetic vision.},
  langid = {english},
  file = {C:\Users\marc_\Zotero\storage\RV2QFECC\Elnabawy e.a. - 2022 - PVGAN a generative adversarial network for object.pdf}
}

@article{fahimiGenerativeAdversarialNetworksBased2021,
  title = {Generative {{Adversarial Networks-Based Data Augmentation}} for {{Brain}}–{{Computer Interface}}},
  author = {Fahimi, Fatemeh and Dosen, Strahinja and Ang, Kai Keng and Mrachacz-Kersting, Natalie and Guan, Cuntai},
  date = {2021-09},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  shortjournal = {IEEE Trans. Neural Netw. Learning Syst.},
  volume = {32},
  number = {9},
  pages = {4039--4051},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2020.3016666},
  url = {https://ieeexplore.ieee.org/document/9177281/},
  urldate = {2024-05-22},
  abstract = {The performance of a classifier in a brain–computer interface (BCI) system is highly dependent on the quality and quantity of training data. Typically, the training data are collected in a laboratory where the users perform tasks in a controlled environment. However, users’ attention may be diverted in reallife BCI applications and this may decrease the performance of the classifier. To improve the robustness of the classifier, additional data can be acquired in such conditions, but it is not practical to record electroencephalogram (EEG) data over several long calibration sessions. A potentially time- and cost-efficient solution is artificial data generation. Hence, in this study, we proposed a framework based on the deep convolutional generative adversarial networks (DCGANs) for generating artificial EEG to augment the training set in order to improve the performance of a BCI classifier. To make a comparative investigation, we designed a motor task experiment with diverted and focused attention conditions. We used an end-to-end deep convolutional neural network for classification between movement intention and rest using the data from 14 subjects. The results from the leaveone subject-out (LOO) classification yielded baseline accuracies of 73.04\% for diverted attention and 80.09\% for focused attention without data augmentation. Using the proposed DCGANs-based framework for augmentation, the results yielded a significant improvement of 7.32\% for diverted attention ( p {$<$} 0.01) and 5.45\% for focused attention ( p {$<$} 0.01). In addition, we implemented the method on the data set IVa from BCI competition III to distinguish different motor imagery tasks. The proposed method increased the accuracy by 3.57\% ( p {$<$} 0.02). This study shows that using GANs for EEG augmentation can significantly improve BCI performance, especially in real-life applications, whereby users’ attention may be diverted.},
  langid = {english},
  file = {C:\Users\marc_\Zotero\storage\9IQ2RYNI\Fahimi e.a. - 2021 - Generative Adversarial Networks-Based Data Augment.pdf}
}

@online{granleyAdaptingBrainLikeNeural2022,
  title = {Adapting {{Brain-Like Neural Networks}} for {{Modeling Cortical Visual Prostheses}}},
  author = {Granley, Jacob and Riedel, Alexander and Beyeler, Michael},
  date = {2022-09-27},
  eprint = {2209.13561},
  eprinttype = {arXiv},
  eprintclass = {cs, q-bio},
  url = {http://arxiv.org/abs/2209.13561},
  urldate = {2024-05-22},
  abstract = {Cortical prostheses are devices implanted in the visual cortex that attempt to restore lost vision by electrically stimulating neurons. Currently, the vision provided by these devices is limited, and accurately predicting the visual percepts resulting from stimulation is an open challenge. We propose to address this challenge by utilizing ‘brain-like’ convolutional neural networks (CNNs), which have emerged as promising models of the visual system. To investigate the feasibility of adapting brain-like CNNs for modeling visual prostheses, we developed a proof-of-concept model to predict the perceptions resulting from electrical stimulation. We show that a neurologically-inspired decoding of CNN activations produces qualitatively accurate phosphenes, comparable to phosphenes reported by real patients. Overall, this is an essential first step towards building brain-like models of electrical stimulation, which may not just improve the quality of vision provided by cortical prostheses but could also further our understanding of the neural code of vision.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  file = {C:\Users\marc_\Zotero\storage\I3UX74V9\Granley e.a. - 2022 - Adapting Brain-Like Neural Networks for Modeling C.pdf}
}

@article{hamiltonVEPEstimationVisual2021,
  title = {{{VEP}} Estimation of Visual Acuity: A Systematic Review},
  shorttitle = {{{VEP}} Estimation of Visual Acuity},
  author = {Hamilton, Ruth and Bach, Michael and Heinrich, Sven P. and Hoffmann, Michael B. and Odom, J. Vernon and McCulloch, Daphne L. and Thompson, Dorothy A.},
  date = {2021-02-01},
  journaltitle = {Documenta Ophthalmologica},
  shortjournal = {Doc Ophthalmol},
  volume = {142},
  number = {1},
  pages = {25--74},
  issn = {1573-2622},
  doi = {10.1007/s10633-020-09770-3},
  url = {https://doi.org/10.1007/s10633-020-09770-3},
  urldate = {2024-07-03},
  abstract = {Visual evoked potentials (VEPs) can be used to measure visual resolution via a spatial frequency (SF) limit as an objective estimate of visual acuity. The aim of this systematic review is to collate descriptions of the VEP SF limit in humans, healthy and disordered, and to assess how accurately and precisely VEP SF limits reflect visual acuity.},
  langid = {english},
  keywords = {ISCEV,Spatial frequency limit,Sweep VEP,Systematic review,Threshold,VEP,Visual acuity},
  file = {C:\Users\marc_\Zotero\storage\C3NJXFKQ\Hamilton e.a. - 2021 - VEP estimation of visual acuity a systematic revi.pdf}
}

@inproceedings{hanDeepLearningBased2021,
  title = {Deep {{Learning}}–{{Based Scene Simplification}} for {{Bionic Vision}}},
  booktitle = {Augmented {{Humans Conference}} 2021},
  author = {Han, Nicole and Srivastava, Sudhanshu and Xu, Aiwen and Klein, Devi and Beyeler, Michael},
  date = {2021-02-22},
  pages = {45--54},
  publisher = {ACM},
  location = {Rovaniemi Finland},
  doi = {10.1145/3458709.3458982},
  url = {https://dl.acm.org/doi/10.1145/3458709.3458982},
  urldate = {2024-06-21},
  eventtitle = {{{AHs}} '21: {{Augmented Humans International Conference}} 2021},
  isbn = {978-1-4503-8428-5},
  langid = {english},
  file = {C:\Users\marc_\Zotero\storage\U5AGNM6D\Han e.a. - 2021 - Deep Learning–Based Scene Simplification for Bioni.pdf}
}

@article{hoLongTermResultsEpiretinal2015,
  title = {Long-{{Term Results}} from an {{Epiretinal Prosthesis}} to {{Restore Sight}} to the {{Blind}}},
  author = {Ho, Allen C. and Humayun, Mark S. and Dorn, Jessy D. and Da Cruz, Lyndon and Dagnelie, Gislin and Handa, James and Barale, Pierre-Olivier and Sahel, José-Alain and Stanga, Paulo E. and Hafezi, Farhad and Safran, Avinoam B. and Salzmann, Joel and Santos, Arturo and Birch, David and Spencer, Rand and Cideciyan, Artur V. and De Juan, Eugene and Duncan, Jacque L. and Eliott, Dean and Fawzi, Amani and Olmos De Koo, Lisa C. and Brown, Gary C. and Haller, Julia A. and Regillo, Carl D. and Del Priore, Lucian V. and Arditi, Aries and Geruschat, Duane R. and Greenberg, Robert J.},
  date = {2015-08},
  journaltitle = {Ophthalmology},
  shortjournal = {Ophthalmology},
  volume = {122},
  number = {8},
  pages = {1547--1554},
  issn = {01616420},
  doi = {10.1016/j.ophtha.2015.04.032},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0161642015004157},
  urldate = {2024-06-10},
  abstract = {Purpose: Retinitis pigmentosa (RP) is a group of inherited retinal degenerations leading to blindness due to photoreceptor loss. Retinitis pigmentosa is a rare disease, affecting only approximately 100 000 people in the United States. There is no cure and no approved medical therapy to slow or reverse RP. The purpose of this clinical trial was to evaluate the safety, reliability, and benefit of the Argus II Retinal Prosthesis System (Second Sight Medical Products, Inc, Sylmar, CA) in restoring some visual function to subjects completely blind from RP. We report clinical trial results at 1 and 3 years after implantation. Design: The study is a multicenter, single-arm, prospective clinical trial. Participants: There were 30 subjects in 10 centers in the United States and Europe. Subjects served as their own controls, that is, implanted eye versus fellow eye, and system on versus system off (native residual vision). Methods: The Argus II System was implanted on and in a single eye (typically the worse-seeing eye) of blind subjects. Subjects wore glasses mounted with a small camera and a video processor that converted images into stimulation patterns sent to the electrode array on the retina. Main Outcome Measures: The primary outcome measures were safety (the number, seriousness, and relatedness of adverse events) and visual function, as measured by 3 computer-based, objective tests. Results: A total of 29 of 30 subjects had functioning Argus II Systems implants 3 years after implantation. Eleven subjects experienced a total of 23 serious device- or surgery-related adverse events. All were treated with standard ophthalmic care. As a group, subjects performed significantly better with the system on than off on all visual function tests and functional vision assessments. Conclusions: The 3-year results of the Argus II trial support the long-term safety profile and benefit of the Argus II System for patients blind from RP. Earlier results from this trial were used to gain approval of the Argus II by the Food and Drug Administration and a CE mark in Europe. The Argus II System is the first and only retinal implant to have both approvals. Ophthalmology 2015;122:1547-1554 ª 2015 by the American Academy of Ophthalmology. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).},
  langid = {english},
  file = {C:\Users\marc_\Zotero\storage\LIVA34FP\Ho e.a. - 2015 - Long-Term Results from an Epiretinal Prosthesis to.pdf}
}

@online{houPredictingTemporalDynamics2024,
  title = {Predicting the {{Temporal Dynamics}} of {{Prosthetic Vision}}},
  author = {Hou, Yuchen and Pullela, Laya and Su, Jiaxin and Aluru, Sriya and Sista, Shivani and Lu, Xiankun and Beyeler, Michael},
  date = {2024-05-01},
  eprint = {2404.14591},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2404.14591},
  url = {http://arxiv.org/abs/2404.14591},
  urldate = {2024-06-20},
  abstract = {Retinal implants are a promising treatment option for degenerative retinal disease. While numerous models have been developed to simulate the appearance of elicited visual percepts ("phosphenes"), these models often either focus solely on spatial characteristics or inadequately capture the complex temporal dynamics observed in clinical trials, which vary heavily across implant technologies, subjects, and stimulus conditions. Here we introduce two computational models designed to accurately predict phosphene fading and persistence under varying stimulus conditions, cross-validated on behavioral data reported by nine users of the Argus II Retinal Prosthesis System. Both models segment the time course of phosphene perception into discrete intervals, decomposing phosphene fading and persistence into either sinusoidal or exponential components. Our spectral model demonstrates state-of-the-art predictions of phosphene intensity over time (r = 0.7 across all participants). Overall, this study lays the groundwork for enhancing prosthetic vision by improving our understanding of phosphene temporal dynamics.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Engineering Finance and Science},
  file = {C\:\\Users\\marc_\\Zotero\\storage\\EX2SGINT\\Hou e.a. - 2024 - Predicting the Temporal Dynamics of Prosthetic Vis.pdf;C\:\\Users\\marc_\\Zotero\\storage\\TU9YAVX3\\2404.html}
}

@article{klistornerAnalysisMultifocalVisual2022,
  title = {Analysis of {{Multifocal Visual Evoked Potentials Using Artificial Intelligence Algorithms}}},
  author = {Klistorner, Samuel and Eghtedari, Maryam and Graham, Stuart L. and Klistorner, Alexander},
  date = {2022-01-10},
  journaltitle = {Translational Vision Science \& Technology},
  shortjournal = {Translational Vision Science \& Technology},
  volume = {11},
  number = {1},
  pages = {10},
  issn = {2164-2591},
  doi = {10.1167/tvst.11.1.10},
  url = {https://doi.org/10.1167/tvst.11.1.10},
  urldate = {2024-07-03},
  abstract = {Clinical trials for remyelination in multiple sclerosis (MS) require an imaging biomarker. The multifocal visual evoked potential (mfVEP) is an accurate technique for measuring axonal conduction; however, it produces large datasets requiring lengthy analysis by human experts to detect measurable responses versus noisy traces. This study aimed to develop a machine-learning approach for the identification of true responses versus noisy traces and the detection of latency peaks in measurable signals.    We obtained 2240 mfVEP traces from 10 MS patients using the VS-1 mfVEP machine, and they were classified by a skilled expert twice with an interval of 1 week. Of these, 2025 (90\%) were classified consistently and used for the study. ResNet-50 and VGG16 models were trained and tested to produce three outputs: no signal, up-sloped signal, or down-sloped signal. Each model ran 1000 iterations with a stochastic gradient descent optimizer with a learning rate of 0.0001.    ResNet-50 and VGG16 had false-positive rates of 1.7\% and 0.6\%, respectively, when the testing dataset was analyzed (n = 612). The false-negative rates were 8.2\% and 6.5\%, respectively, against the same dataset. The latency measurements in the validation and testing cohorts in the study were similar.    Our models efficiently analyze mfVEPs with \&lt;2\% false positives compared with human false positives of \&lt;8\%.    mfVEP, a safe neurophysiological technique, analyzed using artificial intelligence, can serve as an efficient biomarker in MS clinical trials and signal latency measurement.},
  file = {C:\Users\marc_\Zotero\storage\T8GXJXS7\Klistorner e.a. - 2022 - Analysis of Multifocal Visual Evoked Potentials Us.pdf}
}

@article{liuNarrativeReviewCortical2022,
  title = {A Narrative Review of Cortical Visual Prosthesis Systems: The Latest Progress and Significance of Nanotechnology for the Future},
  shorttitle = {A Narrative Review of Cortical Visual Prosthesis Systems},
  author = {Liu, Xi and Chen, Peipei and Ding, Xuemeng and Liu, Anning and Li, Peng and Sun, Cheng and Guan, Huaijin},
  date = {2022-06},
  journaltitle = {Annals of Translational Medicine},
  shortjournal = {Ann Transl Med},
  volume = {10},
  number = {12},
  eprint = {35845476},
  eprinttype = {pmid},
  pages = {716},
  issn = {2305-5839},
  doi = {10.21037/atm-22-2858},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9279795/},
  urldate = {2024-06-12},
  abstract = {Background and Objective We sought to review the latest developments in cortical visual prosthesis (CVP) systems and the significance of nanotechnology for the future. Over the past century, CVP systems have been researched and developed, resulting in various unique surgical and mechanical techniques. Research findings indicate that partial vision recovery is possible, with improvements in coarse target functions and performance in routine activities. Methods This review discusses the architecture and physiology of the visual cortex, the neuroplasticity of the blind brain, and the history of CVP development, and also provides an update on the CVP systems currently being examined in research and clinical trials. Due to advances in nanotechnology, it is possible to make CVPs that are smaller, more efficient, and more biocompatible than ever before. Key Content and Findings Currently, 3 CVPs have entered clinical trials, and several additional systems are undergoing preclinical reviews to determine the safety of the devices for chronic implantation. This development provides the first indication that the area of cortical vision restoration medication may be able to meaningfully benefit blind people. However, several significant technical and biological challenges need to be solved before the gap between artificial and natural eyesight can be reconciled. Rapid breakthroughs in nanotechnology have considerably increased its use in biological domains. Conclusions This paper summarizes the recent progress of CVP in recent years and its future development direction. It is forecasted that nanotechnology can provide better technical support for the development of CVP.},
  pmcid = {PMC9279795},
  file = {C:\Users\marc_\Zotero\storage\X3FCEP5X\Liu e.a. - 2022 - A narrative review of cortical visual prosthesis s.pdf}
}

@inproceedings{liWearableComputerVision2013,
  title = {Wearable {{Computer Vision Systems}} for a {{Cortical Visual Prosthesis}}},
  booktitle = {2013 {{IEEE International Conference}} on {{Computer Vision Workshops}}},
  author = {Li, Wai Ho},
  date = {2013-12},
  pages = {428--435},
  doi = {10.1109/ICCVW.2013.63},
  url = {https://ieeexplore.ieee.org/document/6755929},
  urldate = {2024-06-21},
  abstract = {Cortical visual prostheses produce bionic vision by translating data from a head worn sensor into spatial-temporal patterns of electrical stimulation of a patient's Primary Visual Cortex (V1). The resulting bionic vision has low resolution, poor dynamic range and other limitations. These limitations are unlikely to change in the next decade due to the combined constraints of technology and biology as well as the slow process of medical device certification. This paper discusses ongoing research on Wearable Computer Vision Systems (WCVS) designed for two purposes: Improving the utility of bionic vision and non-invasive evaluation of visual prosthesis on sighted subjects using Simulated Prosthetic Vision (SPV).},
  eventtitle = {2013 {{IEEE International Conference}} on {{Computer Vision Workshops}}},
  keywords = {Cameras,Electrodes,Implants,MVG,Phosphenes,Retina,Transformative Reality,Visual prosthesis,Visualization,Wearable Computer Vision Systems},
  file = {C\:\\Users\\marc_\\Zotero\\storage\\URCFBX3B\\Li - 2013 - Wearable Computer Vision Systems for a Cortical Vi.pdf;C\:\\Users\\marc_\\Zotero\\storage\\BJ7IFB73\\6755929.html}
}

@article{martinez-cagigalBrainComputerInterfaces2021,
  title = {Brain–Computer Interfaces Based on Code-Modulated Visual Evoked Potentials (c-{{VEP}}): A Literature Review},
  shorttitle = {Brain–Computer Interfaces Based on Code-Modulated Visual Evoked Potentials (c-{{VEP}})},
  author = {Martínez-Cagigal, Víctor and Thielen, Jordy and Santamaría-Vázquez, Eduardo and Pérez-Velasco, Sergio and Desain, Peter and Hornero, Roberto},
  date = {2021-11},
  journaltitle = {Journal of Neural Engineering},
  shortjournal = {J. Neural Eng.},
  volume = {18},
  number = {6},
  pages = {061002},
  publisher = {IOP Publishing},
  issn = {1741-2552},
  doi = {10.1088/1741-2552/ac38cf},
  url = {https://dx.doi.org/10.1088/1741-2552/ac38cf},
  urldate = {2024-07-03},
  abstract = {Objective. Code-modulated visual evoked potentials (c-VEP) have been consolidated in recent years as robust control signals capable of providing non-invasive brain–computer interfaces (BCIs) for reliable, high-speed communication. Their usefulness for communication and control purposes has been reflected in an exponential increase of related articles in the last decade. The aim of this review is to provide a comprehensive overview of the literature to gain understanding of the existing research on c-VEP-based BCIs, since its inception (1984) until today (2021), as well as to identify promising future research lines. Approach. The literature review was conducted according to the Preferred Reporting Items for Systematic reviews and Meta-Analysis guidelines. After assessing the eligibility of journal manuscripts, conferences, book chapters and non-indexed documents, a total of 70 studies were included. A comprehensive analysis of the main characteristics and design choices of c-VEP-based BCIs was discussed, including stimulation paradigms, signal processing, modeling responses, applications, etc. Main results. The literature review showed that state-of-the-art c-VEP-based BCIs are able to provide an accurate control of the system with a large number of commands, high selection speeds and even without calibration. In general, a lack of validation in real setups was observed, especially regarding the validation with disabled populations. Future work should be focused toward developing self-paced c-VEP-based portable BCIs applied in real-world environments that could exploit the unique benefits of c-VEP paradigms. Some aspects such as asynchrony, unsupervised training, or code optimization still require further research and development. Significance. Despite the growing popularity of c-VEP-based BCIs, to the best of our knowledge, this is the first literature review on the topic. In addition to providing a joint discussion of the advances in the field, some future lines of research are suggested to contribute to the development of reliable plug-and-play c-VEP-based BCIs.},
  langid = {english},
  file = {C:\Users\marc_\Zotero\storage\5VSYAA8S\Martínez-Cagigal e.a. - 2021 - Brain–computer interfaces based on code-modulated .pdf}
}

@article{merabetWhatBlindnessCan2005,
  title = {What Blindness Can Tell Us about Seeing Again: Merging Neuroplasticity and Neuroprostheses},
  shorttitle = {What Blindness Can Tell Us about Seeing Again},
  author = {Merabet, Lotfi B. and Rizzo, Joseph F. and Amedi, Amir and Somers, David C. and Pascual-Leone, Alvaro},
  date = {2005-01},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  volume = {6},
  number = {1},
  pages = {71--77},
  publisher = {Nature Publishing Group},
  issn = {1471-0048},
  doi = {10.1038/nrn1586},
  url = {https://www.nature.com/articles/nrn1586},
  urldate = {2024-05-17},
  abstract = {Significant progress has been made in the development of visual neuroprostheses to restore vision in blind individuals. Appropriate delivery of electrical stimulation to intact visual structures can evoke patterned sensations of light in those who have been blind for many years. However, success in developing functional visual prostheses requires an understanding of how to communicate effectively with the visually deprived brain in order to merge what is perceived visually with what is generated electrically.},
  langid = {english},
  keywords = {Animal Genetics and Genomics,Behavioral Sciences,Biological Techniques,Biomedicine,general,Neurobiology,Neurosciences},
  file = {C:\Users\marc_\Zotero\storage\B5I84TCW\Merabet e.a. - 2005 - What blindness can tell us about seeing again mer.pdf}
}

@article{mohammedReviewDeepReinforcement2020,
  title = {Review of {{Deep Reinforcement Learning-Based Object Grasping}}: {{Techniques}}, {{Open Challenges}}, and {{Recommendations}}},
  shorttitle = {Review of {{Deep Reinforcement Learning-Based Object Grasping}}},
  author = {Mohammed, Marwan Qaid and Chung, Kwek Lee and Chyi, Chua Shing},
  date = {2020},
  journaltitle = {IEEE Access},
  shortjournal = {IEEE Access},
  volume = {8},
  pages = {178450--178481},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.3027923},
  url = {https://ieeexplore.ieee.org/document/9210095/},
  urldate = {2024-07-03},
  abstract = {The motivation behind our work is to review and analyze the most relevant studies on deep reinforcement learning-based object manipulation. Various studies are examined through a survey of existing literature and investigation of various aspects, namely, the intended applications, techniques applied, challenges faced by researchers and recommendations for minimizing obstacles. This review refers to all relevant articles on deep reinforcement learning-based object manipulation and solutions. The object grasping issue is a major manipulation challenge. Object grasping requires detection systems, methods and tools to facilitate efficient and fast agent training. Several studies have proposed that object grasping and its subtypes are the main elements in dealing with the environment and agent. Unlike other review articles, this review article provides different observations on deep reinforcement learning-based manipulation. The results of this comprehensive review of deep reinforcement learning in the manipulation field may be valuable for researchers and practitioners because they can expedite the establishment of important guidelines.},
  langid = {english},
  file = {C:\Users\marc_\Zotero\storage\G4PPMWFR\Mohammed e.a. - 2020 - Review of Deep Reinforcement Learning-Based Object.pdf}
}

@article{stevensGlobalPrevalenceVision2013,
  title = {Global {{Prevalence}} of {{Vision Impairment}} and {{Blindness}}},
  author = {Stevens, Gretchen A and White, Richard A and Flaxman, Seth R and Price, Holly and Jonas, Jost B and Keeffe, Jill and Leasher, Janet and Naidoo, Kovin and Pesudovs, Konrad and Resnikoff, Serge and Taylor, Hugh and Bourne, Rupert R A},
  date = {2013},
  volume = {120},
  number = {12},
  abstract = {Purpose: Vision impairment is a leading and largely preventable cause of disability worldwide. However, no study of global and regional trends in the prevalence of vision impairment has been carried out. We estimated the prevalence of vision impairment and its changes worldwide for the past 20 years. Design: Systematic review. Participants: A systematic review of published and unpublished population-based data on vision impairment and blindness from 1980 through 2012. Methods: Hierarchical models were fitted fitted to estimate the prevalence of moderate and severe vision impairment (MSVI; defined as presenting visual acuity {$<$}6/18 but !3/60) and the prevalence of blindness (presenting visual acuity {$<$}3/60) by age, country, and year. Main Outcome Measures: Trends in the prevalence of MSVI and blindness for the period 1990 through 2010. Results: Globally, 32.4 million people (95\% confidence interval [CI], 29.4e36.5 million people; 60\% women) were blind in 2010, and 191 million people (95\% CI, 174e230 million people; 57\% women) had MSVI. The agestandardized prevalence of blindness in older adults (!50 years) was more than 4\% in Western Sub-Saharan Africa (6.0\%; 95\% CI, 4.6\%e7.1\%), Eastern Sub-Saharan Africa (5.7\%; 95\% CI, 4.4\%e6.9\%), South Asia (4.4\%; 95\% CI, 3.5\%e5.1\%), and North Africa and the Middle East (4.6\%; 95\% CI, 3.5\%e5.8\%), in contrast to high-income regions with blindness prevalences of 0.4\% or less. The MSVI prevalence in older adults was highest in South Asia (23.6\%; 95\% CI, 19.4\%e29.4\%), Oceania (18.9\%; 95\% CI, 11.8\%e23.7\%), and Eastern and Western Sub-Saharan Africa and North Africa and the Middle East (95\% CI, 15.9\%e16.8\%). The MSVI prevalence was less than 5\% in all 4 high-income regions. The global age-standardized prevalence of blindness and MSVI for older adults decreased from 3.0\% (95\% CI, 2.7\%e3.4\%) worldwide in 1990 to 1.9\% (95\% CI, 1.7\%e2.2\%) in 2010 and from 14.3\% (95\% CI, 12.1\%e16.2\%) worldwide to 10.4\% (95\% CI, 9.5\%e12.3\%), respectively. When controlling for age, women’s prevalence of blindness was greater than men’s in all world regions. Because the global population has increased and aged between 1990 and 2010, the number of blind has increased by 0.6 million people (95\% CI, À5.2 to 5.3 million people). The number with MSVI may have increased by 19 million people (95\% CI, À8 to 72 million people) from 172 million people (95\% CI, 142e198 million people) in 1990. Conclusions: The age-standardized prevalence of blindness and MSVI has decreased in the past 20 years. However, because of population growth and the relative increase in older adults, the blind population has been stable and the population with MSVI may have increased.},
  langid = {english},
  file = {C:\Users\marc_\Zotero\storage\EJPMQCKC\Stevens e.a. - 2013 - Global Prevalence of Vision Impairment and Blindne.pdf}
}

@article{tiwariObjectIdentificationModel2021,
  title = {Object {{Identification Model}} Using {{Deep Reinforcement Machine Learning Concept}} for {{Image}}},
  author = {Tiwari, Saurabh and Veenadhari, S. and Gupta, Sanjeev K.},
  date = {2021-02},
  journaltitle = {IOP Conference Series: Materials Science and Engineering},
  shortjournal = {IOP Conf. Ser.: Mater. Sci. Eng.},
  volume = {1085},
  number = {1},
  pages = {012024},
  publisher = {IOP Publishing},
  issn = {1757-899X},
  doi = {10.1088/1757-899X/1085/1/012024},
  url = {https://dx.doi.org/10.1088/1757-899X/1085/1/012024},
  urldate = {2024-07-03},
  abstract = {This paper presents a model which gives the detailed process of object identification. We need to identify class and location of object in image for completing process of objet identification. Proposed model works on the principal of reinforcement learning which takes action on the basis of rewards and experiences. Normally methods in literature uses sliding window which moves in same direction but proposed algorithm provides a variable mask which moves 360 degree for identifying object using action history vector proposed with RL also not only this work focuses on localization like other work but also used class information with Softmax classification able to classify multiple object in single image with efficient time which is novel. Proposed mask acts as agent and focuses on proposed candidate reason this saves time and works in efficient manner for identification. Agent depends on transformation action and by applying top down reasoning it gives location of object. Classification is done using Softmax classification as we are having features of image by CNN. Reinforcement learning concept used for training of agent and Pascal voc dataset used for testing. Analysis of only 10 to 25 regions is sufficient with proposed work to identify first instance of object. Experiment and performance evaluation shows the efficiency of proposed work.},
  langid = {english},
  file = {C:\Users\marc_\Zotero\storage\S2E58GVB\Tiwari e.a. - 2021 - Object Identification Model using Deep Reinforceme.pdf}
}

@article{towleDevelopmentColorVisual2021,
  title = {Toward the Development of a Color Visual Prosthesis},
  author = {Towle, Vernon L. and Pham, Tuan and McCaffrey, Michael and Allen, Danielle and Troyk, Philip R.},
  date = {2021-02},
  journaltitle = {Journal of Neural Engineering},
  shortjournal = {J. Neural Eng.},
  volume = {18},
  number = {2},
  pages = {023001},
  publisher = {IOP Publishing},
  issn = {1741-2552},
  doi = {10.1088/1741-2552/abd520},
  url = {https://dx.doi.org/10.1088/1741-2552/abd520},
  urldate = {2024-05-11},
  abstract = {Objective. All of the human prosthetic visual systems implanted so far have been achromatic. Schmidt et al (1996 Brain 119 507–22) reported that at low stimulation intensities their subject reported that phosphenes usually had a specific hue, but when the stimulus intensity was increased, they desaturated to white. We speculate here that previous B/W prosthetic systems were unnecessarily over-stimulating the visual cortex to obtain white phosphenes, which may be why unexpected alterations in phosphenes and seizures were not an uncommon occurrence. A color prosthesis would have the advantage of being elicited by lower levels of stimulation, reducing the probability of causing epileptogenic responses. Approach. A ‘hybrid’ mode of stimulation is suggested, involving a combination of B/W and color stimulation, which could provide color information without reducing spatial resolution. Main results. Colors in the real world are spread along intensity and chromatic gradients. Significance. Software implementation strategies are discussed, as are the advantages and challenges for possible color prosthetic systems.},
  langid = {english},
  file = {C:\Users\marc_\Zotero\storage\9FBXMQ3W\Towle e.a. - 2021 - Toward the development of a color visual prosthesi.pdf}
}

@article{vandergrintenBiologicallyPlausiblePhosphene2024,
  title = {Towards Biologically Plausible Phosphene Simulation for the Differentiable Optimization of Visual Cortical Prostheses},
  author = {family=Grinten, given=Maureen, prefix=van der, useprefix=true and family=Ruyter van Steveninck, given=Jaap, prefix=de, useprefix=true and Lozano, Antonio and Pijnacker, Laura and Rueckauer, Bodo and Roelfsema, Pieter and family=Gerven, given=Marcel, prefix=van, useprefix=true and family=Wezel, given=Richard, prefix=van, useprefix=true and Güçlü, Umut and Güçlütürk, Yağmur},
  editor = {Baker, Chris I and Barry, Michael P},
  date = {2024-02-22},
  journaltitle = {eLife},
  volume = {13},
  pages = {e85812},
  publisher = {eLife Sciences Publications, Ltd},
  issn = {2050-084X},
  doi = {10.7554/eLife.85812},
  url = {https://doi.org/10.7554/eLife.85812},
  urldate = {2024-05-14},
  abstract = {Blindness affects millions of people around the world. A promising solution to restoring a form of vision for some individuals are cortical visual prostheses, which bypass part of the impaired visual pathway by converting camera input to electrical stimulation of the visual system. The artificially induced visual percept (a pattern of localized light flashes, or ‘phosphenes’) has limited resolution, and a great portion of the field’s research is devoted to optimizing the efficacy, efficiency, and practical usefulness of the encoding of visual information. A commonly exploited method is non-invasive functional evaluation in sighted subjects or with computational models by using simulated prosthetic vision (SPV) pipelines. An important challenge in this approach is to balance enhanced perceptual realism, biologically plausibility, and real-time performance in the simulation of cortical prosthetic vision. We present a biologically plausible, PyTorch-based phosphene simulator that can run in real-time and uses differentiable operations to allow for gradient-based computational optimization of phosphene encoding models. The simulator integrates a wide range of clinical results with neurophysiological evidence in humans and non-human primates. The pipeline includes a model of the retinotopic organization and cortical magnification of the visual cortex. Moreover, the quantitative effects of stimulation parameters and temporal dynamics on phosphene characteristics are incorporated. Our results demonstrate the simulator’s suitability for both computational applications such as end-to-end deep learning-based prosthetic vision optimization as well as behavioral experiments. The modular and open-source software provides a flexible simulation framework for computational, clinical, and behavioral neuroscientists working on visual neuroprosthetics.},
  keywords = {bionic vision,blindness,cortical stimulation,deep learning,neural implants,neurotechnology,simulated prosthetic vision},
  file = {C:\Users\marc_\Zotero\storage\KCRLQFNK\van der Grinten e.a. - 2024 - Towards biologically plausible phosphene simulatio.pdf}
}

@article{wangNeuroSEENeuromorphicEnergyEfficient2022,
  title = {{{NeuroSEE}}: {{A Neuromorphic Energy-Efficient Processing Framework}} for {{Visual Prostheses}}},
  shorttitle = {{{NeuroSEE}}},
  author = {Wang, Chuanqing and Yang, Jie and Sawan, Mohamad},
  date = {2022-08},
  journaltitle = {IEEE Journal of Biomedical and Health Informatics},
  volume = {26},
  number = {8},
  pages = {4132--4141},
  issn = {2168-2208},
  doi = {10.1109/JBHI.2022.3172306},
  url = {https://ieeexplore.ieee.org/abstract/document/9767670},
  urldate = {2024-05-22},
  abstract = {Visual prostheses with both comprehensive visual signal processing capability and energy efficiency are becoming increasingly demanded in the age of intelligent personal healthcare, particularly with the rise of wearable and implantable devices. To address this trend, we propose NeuroSEE, a neuromorphic energy-efficient processing framework that combines a spike representation encoding technique and a bio-inspired processing method. This framework first utilizes sparse spike trains to represent visual information, and then a bio-inspired spiking neural network (SNN) is adopted to process the spike trains. The SNN model makes use of an IF neuron with multiple spike-firing rates to decrease the energy consumption without compensating for prediction performance. The experimental results indicate that when predicting the response of the primary visual cortex, the framework achieves a state-of-the-art Pearson correlation coefficient performance. Spike-based recording and processing methods simplify the storage and transmission of redundant scene information and complex calculation processes. It could reduce power consumption by 15 times compared with the existing Convolutional neural network (CNN) processing framework. The proposed NeuroSEE framework predicts the response of the primary visual cortex in an energy efficient manner, making it a powerful tool for visual prostheses.},
  eventtitle = {{{IEEE Journal}} of {{Biomedical}} and {{Health Informatics}}},
  keywords = {Age-related macular degeneration,bio-inspired processing,Biological system modeling,Image edge detection,Image restoration,Predictive models,Retina,retinitis pigmentosa,spiking neural network,Visual prostheses,Visual prosthesis,Visualization,wearable devices},
  file = {C\:\\Users\\marc_\\Zotero\\storage\\2D8B8ZHE\\Wang e.a. - 2022 - NeuroSEE A Neuromorphic Energy-Efficient Processi.pdf;C\:\\Users\\marc_\\Zotero\\storage\\EFVS8VGC\\9767670.html}
}
